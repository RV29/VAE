{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 663 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Auto-Encoding Variational Bayes - Final Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"sigmoid function\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    \"\"\"gradient of sigmoid function\"\"\"\n",
    "    return x * (1-x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"tanh function\"\"\"\n",
    "    return np.tanh(x)\n",
    "    \n",
    "def tanh_gradient(x):\n",
    "    \"\"\"gradient of tanh function\"\"\"\n",
    "    return 1-np.power(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Batch(M, trainX, trainy):\n",
    "    \"\"\"randomly sample a mini batch of size M from the training data\"\"\"\n",
    "    \n",
    "    N = trainX.shape[0]\n",
    "    sample = np.random.choice(N,M)\n",
    "    \n",
    "    return trainX[sample], trainy[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random(dx, dm, dz, option = \"xavier\"):\n",
    "    \"\"\"\n",
    "    parameter initialization\n",
    "    xavier initialization for weights\n",
    "    all zero for bias\n",
    "    can be used to initialize all zero variables for ADAM by setting \"option = zeros\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights initialization\n",
    "    if option == \"zeros\":\n",
    "        # only for variables in ADAM algorithm, not to be used for true model parameters\n",
    "        q_W1 = np.zeros((dm, dx))\n",
    "        p_W5 = np.zeros((dx, dm))\n",
    "        q_W2 = np.zeros((dz, dm))\n",
    "        q_W3 = np.zeros((dz, dm))\n",
    "        p_W4 = np.zeros((dm, dz))\n",
    "    elif option == \"xavier\":\n",
    "        bound = np.sqrt(6)/ np.sqrt(dx + dm)\n",
    "        q_W1 = np.random.uniform(-bound, bound, (dm, dx))\n",
    "        p_W5 = np.random.uniform(-bound, bound, (dx, dm))\n",
    "        bound = np.sqrt(6)/ np.sqrt(dm + dz)\n",
    "        q_W2 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        q_W3 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        p_W4 = np.random.uniform(-bound, bound, (dm, dz))\n",
    "    \n",
    "    # bias initialization\n",
    "    q_b1 = np.zeros((dm, 1))\n",
    "    p_b5 = np.zeros((dx, 1))\n",
    "    q_b2 = np.zeros((dz, 1))\n",
    "    q_b3 = np.zeros((dz, 1))\n",
    "    p_b4 = np.zeros((dm, 1))\n",
    "    \n",
    "    W = [q_W1, q_W2, q_W3, p_W4, p_W5]\n",
    "    b = [q_b1, q_b2, q_b3, p_b4, p_b5]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_forward_vec(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation - vectorized version\n",
    "    X: M by dx\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X.T + q_b1 # dm by M\n",
    "    q_h1 = tanh_nb(q_a1) # dm by M\n",
    "    q_mu = q_W2 @ q_h1 + q_b2 # dz by M\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3 # dz by M\n",
    "    q_s2 = np.exp(q_a2) # dz by M\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_vec(q_mu, q_s2, eps):\n",
    "    \"\"\"sample latent variable z - vectorized version\"\"\"\n",
    "    \n",
    "    M, dz = q_mu.shape \n",
    "\n",
    "    return q_mu.reshape(M,1,dz) + np.sqrt(q_s2).reshape(M,1,dz) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward_vec(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation - vectorized version\n",
    "    z: M by L by dz\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = z @ p_W4.T + p_b4.T # M by L by dm\n",
    "    p_h2 = tanh_nb(p_a3) # M by L by dm\n",
    "            \n",
    "    p_a4 = p_h2 @ p_W5.T + p_b5.T # M by L by dx\n",
    "    y = sigmoid_nb(p_a4) # M by L by dx\n",
    "    \n",
    "    return y, p_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_vec(X, y, q_a2, q_mu, q_s2):\n",
    "    \"\"\"target total loss function - to minimize - vectorized version\"\"\"\n",
    "    \n",
    "    M, dx, L = X.shape[0], X.shape[1], y.shape[1]\n",
    "    \n",
    "    # reconstruction loss for each sample of latent variable\n",
    "    loss = -np.sum(X.reshape(M,1,dx) * np.log(y) + (1-X.reshape(M,1,dx))* np.log(1-y)) / L\n",
    "    \n",
    "    # KL divergence/ regularization\n",
    "    loss = (loss + np.sum(np.power(q_mu,2) + q_s2 - q_a2 - 1)/2)/M\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_update(W, b, dW, db, alpha):\n",
    "    \"\"\"update weights and bias for gradient descent\"\"\"\n",
    "    \n",
    "    assert len(W) == len(b) == len(dW) == len(db)\n",
    "    n = len(W)\n",
    "    \n",
    "    for i in range(n):\n",
    "        W[i] = W[i] - alpha * dW[i]\n",
    "        b[i] = b[i] - alpha * db[i]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_nb = jit(sigmoid, nopython=True, cache=True)\n",
    "sigmoid_gradient_nb = jit(sigmoid_gradient, nopython=True, cache=True)\n",
    "tanh_nb = jit(tanh, nopython=True, cache=True)\n",
    "tanh_gradient_nb = jit(tanh_gradient, nopython=True, cache=True)\n",
    "get_Batch_nb = jit(get_Batch, nopython=True)\n",
    "init_random_nb1 = jit(init_random, nopython=True)\n",
    "sample_z_vec_nb = jit(sample_z_vec, nopython=True, cache=True)\n",
    "encoder_forward_vec_nb1 = jit(encoder_forward_vec, nopython=True, cache=True)\n",
    "decoder_forward_vec_nb1 = jit(decoder_forward_vec, cache=True)\n",
    "total_loss_vec_nb = jit(total_loss_vec, nopython=True, cache=True)\n",
    "para_update_nb = jit(para_update, nopython=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def batch_forward_vec_nb2(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one mini batch - full vectorized version\"\"\"\n",
    "    d, M, L, d, dx, dm, dz, d, d = Spec\n",
    "\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    q_h1, q_a2, q_mu, q_s2 = encoder_forward_vec_nb1(X, W, b)\n",
    "    z = sample_z_vec_nb(q_mu, q_s2, eps)\n",
    "    y, p_h2 = decoder_forward_vec_nb1(W, b, z)\n",
    "    \n",
    "    loss = total_loss_vec_nb(X, y, q_a2, q_mu, q_s2)\n",
    "\n",
    "    return y, q_h1, p_h2, q_mu, q_s2, z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps):\n",
    "    \"\"\"\"\n",
    "    batch gradient calculation - vectorized version\n",
    "    not using lists for parameters to enable numba nopython\n",
    "    \n",
    "    inputs:\n",
    "        X: Data [M by dx]\n",
    "        y: Model results [M by L by dx]\n",
    "        q_W1, q_W2, q_W3: Weights for encoder\n",
    "        p_W4, p_W5: Weights for decoder\n",
    "        q_b1, q_b2, q_b3: Bias for encoder\n",
    "        p_b4, p_b5: Bias for decoder\n",
    "        q_h1 (M by dm), p_h2 (M by L by dm): intermediate activation variables\n",
    "        eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz): for sampling latent variables from posterior\n",
    "    \"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "\n",
    "    # back propagation for loss\n",
    "    for iL in range(L):\n",
    "        y_iL = y[:,iL,:] # M by dx\n",
    "        p_h2_iL = p_h2[:,iL,:] # M by dm\n",
    "        z_iL = z[:,iL,:] # M by dz\n",
    "        \n",
    "        L_d4 = y_iL - X # M by dx\n",
    "        dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
    "        dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "        \n",
    "        L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2_iL) # M by dm\n",
    "        dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
    "        dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "        \n",
    "        L_d22 = L_d3 @ p_W4 * eps[iL,] * np.sqrt(q_s2) / 2  # M by dz\n",
    "        dL_dW3 = dL_dW3 + L_d22.T @ q_h1 # dz by dm\n",
    "        dL_db3 = dL_db3 + np.sum(L_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "        \n",
    "        L_d21 = L_d3 @ p_W4 # M by dz\n",
    "        dL_dW2 = dL_dW2 + L_d21.T @ q_h1 # dz by dm\n",
    "        dL_db2 = dL_db2 + np.sum(L_d21, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "        L_d1 = (L_d21 @ q_W2 + L_d22 @ q_W3) * tanh_gradient_nb(q_h1) # M by dm\n",
    "        dL_dW1 = dL_dW1 + L_d1.T @ X # dm by dx\n",
    "        dL_db1 = dL_db1 + np.sum(L_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    # back propagation for regularization\n",
    "    R_d22 = (q_s2 - 1)/2 # M by dz\n",
    "    dR_dW3 = dR_dW3 + R_d22.T @ q_h1 # dz by dm\n",
    "    dR_db3 = dR_db3 + np.sum(R_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "    \n",
    "    R_d21 = q_mu # M by dz\n",
    "    dR_dW2 = dR_dW2 + R_d21.T @ q_h1 # dz by dm\n",
    "    dR_db2 = dR_db2 + np.sum(R_d21, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    R_d1 = (R_d22 @ q_W3 + R_d21 @ q_W2) * tanh_gradient_nb(q_h1) # M by dm\n",
    "    dR_dW1 = dR_dW1 + R_d1.T @ X # dm by dx\n",
    "    dR_db1 = dR_db1 + np.sum(R_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    return dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def train_AEVB(trainX, trainy, nBatch, M = 100, L = 1, std_const = 255, dm = 500, dz = 3, alpha = 0.005, beta1 = 0.9, beta2 = 0.999, eps_stable = 1e-8, W = \"\", b = \"\", nP = 0):\n",
    "    \"\"\"\n",
    "    AEVB model as described in the paper\n",
    "    Diederik P Kingma, Max Welling\n",
    "    Auto-Encoding Variational Bayes (2013).\n",
    "    \n",
    "    Training using ADAM algorithm as described in the paper\n",
    "    Diederik P Kingma, Jimmy Ba\n",
    "    Adam: A Method for Stochastic Optimization (2014).\n",
    "    \n",
    "    Input parameters：\n",
    "    ----------\n",
    "    trainX: array_like\n",
    "            Training dataset inputs.\n",
    "            Dimension: number of sample by dim ...\n",
    "    trainy: array_like\n",
    "            Training dataset labels. \n",
    "            This variable is not currently used in the function. For further developments.\n",
    "    nBatch: integer\n",
    "            Number of mini-batch to train.\n",
    "    M: integer, optional\n",
    "            Size of mini-batch.\n",
    "            Default at 100 as recommended in the paper.\n",
    "    L: integer, optional\n",
    "            Number of latent variable to sample.\n",
    "            Default at 1 as recommended in the paper.\n",
    "    std_const: scalar, optional\n",
    "            Normlizing constant for data.\n",
    "            Currently default at 255 which is usually used for black and white image data.\n",
    "    dm: integer, optional\n",
    "            Dimension for middle layer of the encoder and decoder.\n",
    "            Default at 500 which used for MNIST dataset in the paper.\n",
    "    dz: integer, optional\n",
    "            Dimension for latent variables. \n",
    "            Currently default at 3\n",
    "    alpha: float, optional\n",
    "            Learning rate.\n",
    "            Default at 0.005.\n",
    "    beta1: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 0.9.\n",
    "    beta2: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 0.999.\n",
    "    eps_stable: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 1e-08.\n",
    "    W: list, optional\n",
    "            List of model weights parameters, same format as function output variable W.\n",
    "            In case user wants to start training from existing parameters.\n",
    "    b: list, optional\n",
    "            List of model bias parameters, same format as function output variable b.\n",
    "            In case user wants to start training from existing parameters.\n",
    "    nP: integer, optional\n",
    "            If specified with non-zero number, function will print out \n",
    "            status message after completing every nP batches.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    W: list\n",
    "            List of model weights parameters in the order of: q_W1, q_W2, q_W3, p_W4, p_W5.\n",
    "            q_W1, q_W2, q_W3: weights for Gaussian MLP encoder as specified in paper Appendix C.\n",
    "            p_W4, p_W5: weights for Bernoulli MLP decoder as specified in paper Appendix C\n",
    "    b: list\n",
    "            List of model bias parameters in the order of q_b1, q_b2, q_b3, p_b4, p_b5.\n",
    "            q_b1, q_b2, q_b3: bias for Gaussian MLP encoder as specified in paper Appendix C.\n",
    "            p_b4, p_b5: weights for Bernoulli MLP decoder as specified in paper Appendix C.\n",
    "    loss: array-like\n",
    "            Array which stores total loss for each mini-batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    dx = trainX.shape[1]\n",
    "    Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random_nb1(dx, dm, dz)\n",
    "    \n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "        X = batchX / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = batch_forward_vec_nb2(Spec, X, W, b, eps)\n",
    "        \n",
    "        dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 =\\\n",
    "        grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)\n",
    "        \n",
    "        dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "        db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update_nb(W, b, dW, db, alpha)\n",
    "        \n",
    "        if (nP != 0) and (iB+1) % nP == 0:\n",
    "            print(\"Batch \" + str(iB+1) + \" completed.\")\n",
    "\n",
    "    return W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(trainX, trainy, W, b, std_const = 255):\n",
    "    \"\"\"\n",
    "    Randomly sample 9 figures from training data, reconstruct based on \n",
    "    user specified model parameters and plot for comparison.\n",
    "    \n",
    "    Input parameters：\n",
    "    ----------\n",
    "    trainX: array_like\n",
    "            Training dataset inputs.\n",
    "            Dimension: number of sample by dim1 by dim2 ...\n",
    "    trainy: array_like\n",
    "            Training dataset labels. \n",
    "            This variable is not currently used in the function. For further developments.\n",
    "    W: list\n",
    "            List of model weights parameters, same format as train_AEVB function output variable W.\n",
    "    b: list\n",
    "            List of model bias parameters, same format as train_AEVB function output variable b.\n",
    "    std_const: integer, optional\n",
    "            Normlizing constant to reconstruct data.\n",
    "            Currently default at 255 which is usually used for black and white image data.            \n",
    "    \n",
    "    Output:\n",
    "    ----------\n",
    "    9 random sampled figures from training data and the model-reconstructed ones for comparison\n",
    "    \"\"\"\n",
    "\n",
    "    dx = trainX.shape[1]\n",
    "    M = 9\n",
    "    L = 1\n",
    "    dz, dm = W[1].shape[0], W[1].shape[1]\n",
    "    Spec = [1, M, L, 255, dx, dm, dz, 0.005, 0]\n",
    "\n",
    "    batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "    X = batchX / std_const\n",
    "    eps = np.zeros((L, dz))\n",
    "\n",
    "    y, q_h1, p_h2, q_mu, q_s2, z, loss = batch_forward_vec_nb2(Spec, X, W, b, eps)\n",
    "    \n",
    "    \n",
    "    batchX = batchX.reshape(M, 28, 28)\n",
    "    \n",
    "    for i in range(M):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(batchX[i], cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(M):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(y[i,L-1,].reshape(Xdim1, Xdim2) * std_const, cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to plot random 9 figures from MNIST dataset based on our own learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "\n",
    "# for training, change dimensions\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "trainX = trainX.reshape(trainX.shape[0], Xdim1*Xdim2)\n",
    "\n",
    "def get_para(filename):\n",
    "    \"\"\"read parameters from file\"\"\"\n",
    "    file = open(filename, 'rb')\n",
    "    dict = pickle.load(file)\n",
    "    return dict\n",
    "\n",
    "filename = \"Files/2020_04_30_Final_Model_dz5_100k_parameter.txt\"\n",
    "para = get_para(filename)\n",
    "\n",
    "W = para['W']\n",
    "b = para['b']\n",
    "\n",
    "plot_samples(trainX, trainy, W, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
